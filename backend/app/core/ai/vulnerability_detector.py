import tensorflow as tf
import numpy as np
from typing import List, Dict, Any, Optional, Union
import re
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import joblib
import os

class VulnerabilityDetector:
    def __init__(self):
        self.vulnerability_types = {
            'web': [
                'sql_injection', 'xss', 'csrf', 'ssrf',
                'authentication_bypass', 'authorization_bypass',
                'insecure_headers', 'information_disclosure'
            ],
            'api': [
                'broken_authentication', 'broken_authorization',
                'data_exposure', 'rate_limiting', 'input_validation',
                'security_misconfiguration'
            ],
            'mobile': [
                'insecure_data_storage', 'weak_cryptography',
                'insecure_communication', 'code_injection',
                'reverse_engineering_vulnerability'
            ],
            'source_code': [
                'hardcoded_secrets', 'insecure_configuration',
                'unsafe_deserialization', 'memory_corruption',
                'code_injection'
            ],
            'blockchain': [
                'reentrancy', 'overflow_underflow',
                'front_running', 'access_control',
                'gas_optimization', 'logic_vulnerabilities'
            ]
        }

        self.models = self._load_or_create_models()
        self.vectorizers = self._initialize_vectorizers()

    def _load_or_create_models(self) -> Dict[str, tf.keras.Model]:
        models = {}
        for scanner_type, vulnerabilities in self.vulnerability_types.items():
            model = tf.keras.Sequential([
                tf.keras.layers.Input(shape=(1000,)),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.Dense(256, activation='relu'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Dense(len(vulnerabilities), activation='sigmoid')
            ])

            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy', tf.keras.metrics.AUC()]
            )
            models[scanner_type] = model

        return models

    def _initialize_vectorizers(self) -> Dict[str, TfidfVectorizer]:
        vectorizers = {}
        for scanner_type in self.vulnerability_types.keys():
            vectorizers[scanner_type] = TfidfVectorizer(
                max_features=1000,
                ngram_range=(1, 3),
                stop_words='english'
            )
        return vectorizers

    async def analyze_web_vulnerabilities(self, url: str, found_vulnerabilities: List[Dict]) -> List[Dict]:
        """Analyze web-specific vulnerabilities using AI"""
        features = self._extract_web_features(url, found_vulnerabilities)
        predictions = self.models['web'].predict(features)
        return self._process_predictions('web', predictions[0], 0.5)

    async def analyze_api_vulnerabilities(self, endpoint: str, response_data: Dict) -> List[Dict]:
        """Analyze API-specific vulnerabilities using AI"""
        features = self._extract_api_features(endpoint, response_data)
        predictions = self.models['api'].predict(features)
        return self._process_predictions('api', predictions[0], 0.6)

    async def analyze_mobile_vulnerabilities(self, app_binary: bytes, metadata: Dict) -> List[Dict]:
        """Analyze mobile app vulnerabilities using AI"""
        features = self._extract_mobile_features(app_binary, metadata)
        predictions = self.models['mobile'].predict(features)
        return self._process_predictions('mobile', predictions[0], 0.7)

    async def analyze_source_code_vulnerabilities(self, code: str, language: str) -> List[Dict]:
        """Analyze source code vulnerabilities using AI"""
        features = self._extract_source_code_features(code, language)
        predictions = self.models['source_code'].predict(features)
        return self._process_predictions('source_code', predictions[0], 0.6)

    async def analyze_blockchain_vulnerabilities(self, contract_code: str, bytecode: str) -> List[Dict]:
        """Analyze blockchain/smart contract vulnerabilities using AI"""
        features = self._extract_blockchain_features(contract_code, bytecode)
        predictions = self.models['blockchain'].predict(features)
        return self._process_predictions('blockchain', predictions[0], 0.7)

    def _extract_web_features(self, url: str, found_vulnerabilities: List[Dict]) -> np.ndarray:
        """Extract features from web content and found vulnerabilities"""
        text_content = ' '.join([
            str(vuln.get('description', '')) + ' ' +
            str(vuln.get('type', '')) + ' ' +
            str(vuln.get('payload', ''))
            for vuln in found_vulnerabilities
        ])
        return self._vectorize_text('web', text_content)

    def _extract_api_features(self, endpoint: str, response_data: Dict) -> np.ndarray:
        """Extract features from API endpoint and response"""
        text_content = f"{endpoint} {json.dumps(response_data)}"
        return self._vectorize_text('api', text_content)

    def _extract_mobile_features(self, app_binary: bytes, metadata: Dict) -> np.ndarray:
        """Extract features from mobile app binary and metadata"""
        text_content = f"{str(metadata)} {app_binary[:1000].hex()}"
        return self._vectorize_text('mobile', text_content)

    def _extract_source_code_features(self, code: str, language: str) -> np.ndarray:
        """Extract features from source code"""
        # Remove comments and normalize whitespace
        code = re.sub(r'/\*[\s\S]*?\*/|//.*', '', code)
        code = re.sub(r'\s+', ' ', code)
        text_content = f"{language} {code}"
        return self._vectorize_text('source_code', text_content)

    def _extract_blockchain_features(self, contract_code: str, bytecode: str) -> np.ndarray:
        """Extract features from smart contract code and bytecode"""
        text_content = f"{contract_code} {bytecode}"
        return self._vectorize_text('blockchain', text_content)

    def _vectorize_text(self, scanner_type: str, text: str) -> np.ndarray:
        """Convert text to feature vector using appropriate vectorizer"""
        # Fit vectorizer if not already fitted
        if not hasattr(self.vectorizers[scanner_type], 'vocabulary_'):
            self.vectorizers[scanner_type].fit([text])

        features = self.vectorizers[scanner_type].transform([text]).toarray()
        return features

    def _process_predictions(self, scanner_type: str, predictions: np.ndarray, threshold: float) -> List[Dict]:
        """Process model predictions into vulnerability findings"""
        findings = []
        for i, prob in enumerate(predictions):
            if prob > threshold:
                vuln_type = self.vulnerability_types[scanner_type][i]
                findings.append({
                    'type': vuln_type,
                    'confidence': float(prob),
                    'severity': self._determine_severity(prob),
                    'description': self._get_vulnerability_description(scanner_type, vuln_type),
                    'recommendations': self._get_vulnerability_recommendations(scanner_type, vuln_type)
                })
        return findings

    def _determine_severity(self, confidence: float) -> str:
        """Determine vulnerability severity based on confidence score"""
        if confidence > 0.8:
            return 'critical'
        elif confidence > 0.6:
            return 'high'
        elif confidence > 0.4:
            return 'medium'
        return 'low'

    def _get_vulnerability_description(self, scanner_type: str, vuln_type: str) -> str:
        """Get detailed description for a vulnerability type"""
        # This would typically load from a vulnerability database
        # For now, return a basic description
        return f"Potential {vuln_type} vulnerability detected in {scanner_type} scan"

    def _get_vulnerability_recommendations(self, scanner_type: str, vuln_type: str) -> List[str]:
        """Get remediation recommendations for a vulnerability type"""
        # This would typically load from a vulnerability database
        # For now, return basic recommendations
        return [f"Review and fix {vuln_type} vulnerability according to security best practices"]

    async def train_model(self, scanner_type: str, training_data: List[Dict[str, Any]]) -> None:
        """Train the model for a specific scanner type"""
        if scanner_type not in self.models:
            raise ValueError(f"Invalid scanner type: {scanner_type}")

        # Extract features and labels from training data
        X = []
        y = []
        for item in training_data:
            text = item.get('text', '')
            labels = item.get('labels', [])

            features = self._vectorize_text(scanner_type, text)
            X.append(features[0])

            # Convert labels to multi-hot encoding
            label_vector = np.zeros(len(self.vulnerability_types[scanner_type]))
            for label in labels:
                if label in self.vulnerability_types[scanner_type]:
                    idx = self.vulnerability_types[scanner_type].index(label)
                    label_vector[idx] = 1
            y.append(label_vector)

        X = np.array(X)
        y = np.array(y)

        # Train the model
        self.models[scanner_type].fit(
            X, y,
            epochs=10,
            batch_size=32,
            validation_split=0.2,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=3,
                    restore_best_weights=True
                )
            ]
        )
